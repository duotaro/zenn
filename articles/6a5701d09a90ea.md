---
title: "Gemini API × Streamlit で作るチャットボット開発入門 - モデル選定から始めるLLMOps"
emoji: "🦁"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ['LLMOps','Streamlit','Gemini','Langfuse']
published: true
---

# Gemini API × Streamlit で作るチャットボット開発入門 - モデル選定から始めるLLMOps

## はじめに：なぜ「Gemini 2.5 Flash」なのか？

個人開発やプロトタイピングにおいて、どのLLMを選択すべきか。
OpenAIのGPT-4oか、AnthropicのClaude 3.5か、それともGoogleのGeminiか。

結論から言えば、**「万能な正解」は存在しない。**
「複雑な論理推論が必要ならGPT-4」「クリエイティブな文章ならClaude」「大量のトークンを安く処理したいならGemini」といった具合に、**用途と目的に応じて最適なモデルを使い分けることこそが重要**だからだ。

そして、この **「モデル選定（Model Selection）」** という意思決定プロセスそのものが、実は **LLMOps（LLM Operations）** の最初の役割でもある。

今回は、あえて **Gemini 2.5 Flash** を採用する。理由は以下の2点だ。

1. **圧倒的なコストパフォーマンス（無料枠の存在）:** 個人開発において、APIコストを気にせず試行錯誤できる点は最強の武器になる。
2. **十分な実用性能:** 軽量モデルでありながら、単純なチャットボット用途では十分なレスポンス速度と精度を持つ。

この記事では、Streamlitを使ったチャットアプリの構築から、**Langfuse** を用いた「評価・改善ループ」の構築までを解説する。モデル選びから運用まで、一貫したLLMOpsの視点を身につけよう。

## 開発環境のセットアップ

まずは必要なライブラリをインストールする。Geminiを利用するための `langchain-google-genai` と、可観測性を担保する `langfuse` がキーとなる。

**動作確認済み環境:**
- Python 3.9
- langchain==0.3.27
- streamlit==1.50.0
- langfuse==3.7.0

```bash
pip install streamlit langchain-google-genai langfuse python-dotenv
# 必要に応じてバージョン指定を行ってください
# pip install langchain==0.3.27 streamlit==1.50.0 langfuse==3.7.0 ...
```

次に、プロジェクトルートに `.env` ファイルを作成し、APIキーを設定する。

```env
# Google AI Studioで取得
GOOGLE_API_KEY=AIzaSy...

# Langfuse Project Settingsで取得
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_HOST=https://cloud.langfuse.com # 自ホストの場合は変更

```

## Geminiチャットボットの作成 (Streamlit)

まずは、Langfuseなしの純粋なGeminiチャットボットを実装する。
Streamlitの `st.chat_message` と `st.session_state` を使えば、履歴保持機能付きのUIが簡単に作れる。

`app.py` として以下を作成する。ここで `gemini-2.5-flash` を指定しているが、もし要件が変わり「もっと高い推論能力が必要」となれば、ここを `gemini-pro` や `gpt-4o` に差し替える判断が必要になる。

```python
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage

# 環境変数の読み込み
load_dotenv()

st.title("Gemini 2.5 Flash Chatbot")

# 1. モデルの初期化
# ここでのモデル選定がLLMOpsの第一歩。
# 今回は「コスト」と「速度」を優先してFlashモデルを選択。
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7
)

# 2. 会話履歴の初期化
if "messages" not in st.session_state:
    st.session_state.messages = []

# 3. 履歴の表示
for message in st.session_state.messages:
    with st.chat_message(message.type):
        st.markdown(message.content)

# 4. チャット入力と応答生成
if prompt := st.chat_input("何でも聞いてください"):
    # ユーザーの入力を表示・保存
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append(HumanMessage(content=prompt))

    # AIの応答をストリーミング表示
    with st.chat_message("assistant"):
        response_stream = llm.stream(st.session_state.messages)
        response_text = st.write_stream(response_stream)
    
    # AIの応答を保存
    st.session_state.messages.append(AIMessage(content=response_text))

```

## Langfuseの導入とトレーシング

ここからが本題だ。
選定したモデル（今回はGemini 2.5 Flash）が、実際に**期待通りのパフォーマンスを出しているか**を監視する必要がある。

「思ったより回答精度が低い」「実は無料枠を超えてコストがかかっている」といった事実は、ログを取らなければ気づけない。そのためにLangfuseを導入する。

```python
# 追加インポート
from langfuse.langchain import CallbackHandler

# --- 中略 ---

# Langfuseハンドラーの初期化（環境変数が読み込まれていれば引数不要）
langfuse_handler = CallbackHandler()

# --- チャット応答生成部分を修正 ---

if prompt := st.chat_input("何でも聞いてください"):
    # ... (ユーザー入力処理は同じ) ...

    with st.chat_message("assistant"):
        # configにcallbacksを渡すだけ！
        response_stream = llm.stream(
            st.session_state.messages,
            config={"callbacks": [langfuse_handler]} 
        )
        response_text = st.write_stream(response_stream)
    
    # ... (保存処理は同じ) ...

```

これで、Langfuseのダッシュボードに全ての会話ログ（トレース）が送信される。
もしここで「Gemini 2.5 Flashだと回答の質が悪い」というデータが集まれば、 **「コストをかけてでもProモデルに変更する」** という意思決定（LLMOps）をデータドリブンに行えるようになる。

![](/images/6a5701d09a90ea/6a5701d09a90ea-01.png)

:::message
**Tips:**
Langfuseの管理画面では、トークン消費量やレイテンシだけでなく、実際のプロンプトと回答のセットを一覧で確認できる。これがモデル再選定の重要な根拠となる。
:::

## プロンプト管理と実験

コード内にSystem Promptをハードコードするのは悪手だ。PMやドメインエキスパートがプロンプトを修正するたびにデプロイが必要になるからだ。
Langfuseの「Prompt Management」機能を使おう。

:::message alert
**注意:**
このコードを実行する前に、Langfuseのダッシュボード上で `chatbot-system-prompt` という名前のプロンプトを作成しておく必要があります。作成されていない場合、404エラーが発生します。
:::

```python
from langfuse import Langfuse

# Langfuseクライアントの初期化
langfuse = Langfuse()

# Langfuseサーバーからプロンプトを取得（キャッシュも効く）
# 名前: "chatbot-system-prompt", バージョン: latest
prompt_template = langfuse.get_prompt("chatbot-system-prompt")

# LangChain形式のプロンプトオブジェクトに変換
# 変数がある場合はここで注入可能
chain = prompt_template.compile() | llm

# stream実行時
response_stream = chain.stream(
    {"input": prompt, "history": st.session_state.messages},
    config={"callbacks": [langfuse_handler]}
)

```

## 回答の評価と改善

最後に、ユーザーフィードバック（Good / Bad）を収集する仕組みを作る。
「ユーザーが満足しなかった回答」だけをフィルタリングして分析するためだ。

```python
# LangfuseハンドラーからTrace IDを取得してセッションに保存する工夫
# 注意: stream処理完了後にTrace IDを確保する必要がある

# ... stream処理後 ...
# 修正: v3.7.0以降では get_trace_id() ではなく last_trace_id 属性を参照する
current_trace_id = langfuse_handler.last_trace_id
st.session_state["last_trace_id"] = current_trace_id

# 評価ボタンの設置
col1, col2 = st.columns(2)
with col1:
    if st.button("👍 Good"):
        langfuse.score(
            trace_id=st.session_state["last_trace_id"],
            name="user-feedback",
            value=1,
            comment="User liked this response"
        )
        st.toast("評価を送信しました！")

with col2:
    if st.button("👎 Bad"):
        langfuse.score(
            trace_id=st.session_state["last_trace_id"],
            name="user-feedback",
            value=0,
            comment="User disliked this response"
        )
        st.toast("フィードバックありがとうございます")

```

このフィードバックループが回って初めて、「Gemini 2.5 Flashで十分なのか、それともモデルを変えるべきか」という問いに自信を持って答えられるようになる。

## まとめ

LLMアプリ開発において、「どのモデルを使うか」はスタート地点であり、ゴールではない。

1. **選定する:** 用途に合わせてモデルを選ぶ（今回は無料枠のGemini Flashを採用）
2. **作る:** Streamlit で爆速開発
3. **測る:** Langfuse でログと評価を集める
4. **判断する:** データに基づいて、モデルの変更やプロンプト改善を行う

この一連のサイクル（LLMOps）を回せるエンジニアこそが、これからのAI時代に求められる人材だ。

---

技術的な実装は以上です。なぜエンジニアが「動くもの」だけでなく「計測できるもの」を作るべきなのか？ そのキャリア的な戦略や背景については、以下のブログにまとめました。

[【2025年版】LLMを「おもちゃ」で終わらせない。GenAIOpsエンジニアへの現実的なロードマップ](https://it-innovation.hatenablog.com/entry/2025/12/25/114503)
