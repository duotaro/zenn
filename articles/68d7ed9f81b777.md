---
title: "LangChainのParentDocumentRetriever完全実装ガイド：RAGの検索精度を「文脈」で殴って解決する"
emoji: "😸"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ['LangChain','ParentDocumentRetriever','RAG','ベクトル検索']
published: true
---


# LangChainのParentDocumentRetriever完全実装ガイド：RAGの検索精度を「文脈」で殴って解決する

## はじめに：RAGの「チャンク分割」ジレンマ

RAG（Retrieval-Augmented Generation）を構築する際、エンジニアを最も悩ませるのが **「チャンクサイズ（Chunk Size）の調整」** だ。

* **チャンクを小さくすると**
検索ヒット率は上がるが、前後の文脈（Context）が欠落する。結果、LLMが断片的な情報から推論することになり、回答が的外れになる。
* **チャンクを大きくすると**
文脈は保たれるが、検索のベクトル表現がぼやけ、本当に必要な情報がヒットしなくなる（ノイズが増える）。

このトレードオフを解消するために、`chunk_size=500` や `1000` で無限にパラメータチューニングを繰り返すのは時間の無駄だ。

結論から言う。LangChainの **`ParentDocumentRetriever`** を使え。

この記事では、検索精度と文脈維持を両立させるこの機能の仕組みと、実務レベルの実装コードを解説する。

:::message
**この記事の対象読者**

* 単純なRAGを実装したが、回答精度が上がらず頭打ちになっている人
* チャンクサイズの調整作業に疲れ果てた人
:::

## ParentDocumentRetrieverとは？

仕組みは単純だ。「検索に使うデータ」と「LLMに渡すデータ」を分離する。

1. **Child Documents（検索用）**
ドキュメントを細かく分割したもの。ベクトル化して検索に使用する。ピンポイントでヒットさせるために小さくする。
2. **Parent Documents（生成用）**
Childが属する「親」のドキュメント（または大きめのチャンク）。検索でChildがヒットしたら、実際にLLMに渡すのはこのParentの方だ。

*(出典: LangChain Documentation)*

これにより、 **「細かい粒度で検索し、広い文脈で回答する」** という理想的な挙動が可能になる。

## 実装環境と前提条件

以下のライブラリ構成を前提とする。

```bash
pip install langchain langchain-openai langchain-community faiss-cpu

```

* LangChain v0.1系以上
* VectorStore: FAISS（ローカル検証用）
* LLM: OpenAI (GPT-4o or GPT-3.5-turbo)

## 【コード解説】InMemoryStoreでの最小実装

まずは理解のために、Key-Valueストアをメモリ上（InMemoryStore）に持つ最小構成で実装する。

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

# 1. ドキュメントの読み込み
loaders = [TextLoader('./data/technical_spec.txt')]
docs = []
for loader in loaders:
    docs.extend(loader.load())

# 2. Embeddingモデルの準備
embeddings = OpenAIEmbeddings()

# 3. TextSplitterの定義（ここが重要）
# Parent用：大きな文脈（例：2000文字）
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
# Child用：細かい検索単位（例：400文字）
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

# 4. ストアの初期化
# ベクトル検索用（Childを格納）
vectorstore = FAISS.from_texts(["Init"], embeddings)
# 元ドキュメント保存用（Parentを格納、IDで紐付け）
store = InMemoryStore()

# 5. Retrieverの構築
retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter, # 小さいチャンクを作る
    parent_splitter=parent_splitter, # 大きいチャンクを作る
    # parent_splitterを指定しない場合、DocumentそのものがParentとして扱われる
)

# 6. ドキュメントの追加（この時点でSplitとIndexingが走る）
retriever.add_documents(docs, ids=None)

# 7. 検索実行
# 実際に返ってくるのは Child ではなく Parent（大きいチャンク）である
sub_docs = retriever.invoke("特定の仕様に関する質問")

print(f"Retrieved Context Length: {len(sub_docs[0].page_content)}")
print(sub_docs[0].page_content)

```

このコードを実行すると、ベクトル検索自体は細かいチャンクで行われるが、結果として返ってくる `sub_docs` は大きなチャンク（Parent）であることが確認できるはずだ。

## 【コード解説】本番を見据えた永続化

上記の `InMemoryStore` は、プロセスが落ちるとParentドキュメントのデータが消える。これでは実務で使えない。
本番環境では、Redisなどを用いた永続化が必要だ。`BaseStore` インターフェースに対応していれば何でも良いが、ここでは `LocalFileStore` またはRedisを想定したパターンを示す。

### Redisを使用する場合の変更点

`langchain-redis` や `upstash-redis` を利用してストアを永続化する。

```python
# InMemoryStore の代わりに RedisStore を使う例
from langchain_community.storage import RedisStore

# Redisへの接続URL
redis_url = "redis://localhost:6379"

# storeの部分だけ差し替える
store = RedisStore(
    redis_url=redis_url,
    namespace="parent_docs"
)

# Retrieverの定義は同じ
retriever = ParentDocumentRetriever(
    vectorstore=vectorstore, # VectorStoreも本来はChromaやPinecone等の永続化対応を使う
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)

```

:::message alert
**注意点**
VectorStoreとDocStoreの両方を永続化しないと、再起動後に整合性が取れなくなる。必ずセットで永続化設計を行うこと。
:::

## Before/After 精度の比較（想定ログ）

実際に社内ドキュメント（API仕様書）で検証した際の違いだ。

### 課題：認証エラー時の挙動を知りたい

ドキュメントには「認証エラー(401)」の記述と、「バリデーションエラー(400)」の記述が離れた場所にあり、かつエラーハンドリングの共通ロジックが別の章にある構成。

**Before（通常のChunking）**

* **取得内容:** 「401エラーが発生します」という一文だけを含むチャンクがヒット。
* **LLM回答:** 「401エラーが発生するという記述があります。」（解決策を含まない）

**After（ParentDocumentRetriever）**

* **取得内容:** 「401エラー」の単語でChildがヒットしたが、リトリーブされたのはその周辺を含む親チャンク（共通エラーハンドリングの章全体）。
* **LLM回答:** 「401エラーが発生した場合、システムは自動的にリトライ処理を行います。それでも失敗する場合は管理者へ通知されます。」（文脈を補完して回答）

検索の鋭さはそのままに、回答に必要な「背景情報」を丸ごとLLMに渡せていることがわかる。

## おわりに：技術選定の「裏側」はブログで

`ParentDocumentRetriever` は強力だが、銀の弾丸ではない。
Parentチャンクが大きすぎればLLMのコンテキストウィンドウを圧迫し、課金額（Input Token）も増大する。結局のところ、**「精度」と「コスト」と「速度」のバランスをどこで取るか**という、システム設計の課題に行き着く。

今回はLangChainの特定機能に絞って解説したが、これを実運用システム（GenAIOps）としてどう組み上げるか、あるいはこういった技術選定の泥臭い過程を経てエンジニアとしてどうキャリアを築くかについては、Zennのスコープを超えるため個人のブログにまとめている。

技術の「点」ではなく、アーキテクチャやキャリアという「線」で理解したい方は、以下の記事を参照してほしい。

[【2025年版】LLMを「おもちゃ」で終わらせない。GenAIOpsエンジニアへの現実的なロードマップ](https://it-innovation.hatenablog.com/entry/2025/12/25/114503)
